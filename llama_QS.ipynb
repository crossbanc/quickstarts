{
 "cells":[
  {
   "cell_type":"code",
   "source":[
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch import nn\n",
    "from torch.nn import functional as f\n",
    "from collections import OrderedDict "
   ],
   "execution_count":null,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "node_id":"GtN3XAT7vntAc7Jvp53Qg1",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "# Simplified model parameters \n",
    "params = {     \n",
    "    \n",
    "    'context_window': 16,\n",
    "    'd_model': 128,          \n",
    "    'log_interval': 10,      \n",
    "    'batch_size': 32, \n",
    "    'n_heads': 8,\n",
    "    'n_layers': 4,\n",
    "    'epochs': 10000\n",
    "}\n",
    "\n",
    "learnenglish = \"your_learn_english.txt\"\n",
    "lines = open(learnenglish, 'r').read()\n",
    "# Create a sorted list of unique characters in the dataset\n",
    "vocab = sorted(list(set(lines)))"
   ],
   "execution_count":null,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "node_id":"ZW6A1Dg5fS6X3lzJAvQZ9r",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "# Mapping integers to characters (itos)\n",
    "itos = {i: ch for i, ch in enumerate(vocab)}\n",
    "# Mapping characters to integers (stoi)\n",
    "stoi = {ch: i for i, ch in enumerate(vocab)}"
   ],
   "execution_count":null,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "node_id":"6HAOL9rjqr8gjkh2on8fgK",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "# Encode function: Converts a string to a list of integers using the mapping stoi\n",
    "def encode(s):\n",
    "    return [stoi[ch] for ch in s]\n",
    "\n",
    "# Decode function: Converts a list of integers back to a string using the mapping itos\n",
    "def decode(l):\n",
    "    return ''.join([itos[i] for i in l])\n",
    "\n",
    "# Alternatively, use the sentencepiece python wrapper but you will need to modify get_batches function. \n",
    "# https:\/\/github.com\/google\/sentencepiece\/blob\/master\/python\/README.md\n",
    "\n",
    "# In machine learning or deep learning projects, such train-test splits are crucial for developing and evaluating models, \n",
    "# and the same principle applies here. Depending on the use-case, the approach may be different."
   ],
   "execution_count":null,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "node_id":"6VZvRI86d0j6mWvl63p6CY",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "dataset = torch.tensor(encode(lines), dtype=torch.int8)"
   ],
   "execution_count":null,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "node_id":"QlYHrIZthQb5MQJFE9lw13",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "# Function to get batches for training, validation, or testing \n",
    "def get_batches(data, split, batch_size, context_window):\n",
    "    # Split the dataset into training, validation, and test sets\n",
    "    trainset = data[:int(.8 * len(data))]\n",
    "    val = data[int(.8 * len(data)): int(.9 * len(data))]\n",
    "    testset = data[int(.9 * len(data)):]\n",
    "\n",
    "    # Determine which split to use\n",
    "    batch_data = trainset\n",
    "    if split == 'val':\n",
    "        batch_data = val\n",
    "    if split == 'test':\n",
    "        batch_data = testset\n",
    "\n",
    "    # Pick random starting points within the data\n",
    "    ix = torch.randint(0, batch_data.size(0) - context_window - 1, (batch_size,))\n",
    "\n",
    "    # Create input sequences (x) and corresponding target sequences (y)\n",
    "    x = torch.stack([batch_data[i:i+context_window] for i in ix]).long()\n",
    "    y = torch.stack([batch_data[i+1:i+context_window+1] for i in ix]).long()\n",
    "\n",
    "    return x, y"
   ],
   "execution_count":null,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "node_id":"lebbbym4TsbANA51ZDqphx",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "@torch.no_grad()  \n",
    "def evaluate_loss(model, config):\n",
    "\n",
    "    out = {}\n",
    "    model.eval()\n",
    "\n",
    "    # Iterate through training and validation splits\n",
    "    for split in [\"train\", \"val\"]:\n",
    "\n",
    "        losses = []\n",
    "        # Generate 10 batches for evaluation\n",
    "        for _ in range(10):\n",
    "            # Get input sequences (xb) and target sequences (yb)\n",
    "            xb, yb = get_batches(dataset, split, config['batch_size'], config['context_window'])\n",
    "            \n",
    "            # Perform model inference and calculate the loss\n",
    "            _, loss = model(xb, yb)\n",
    "            \n",
    "            # Append the loss to the list\n",
    "            losses.append(loss.item())\n",
    "\n",
    "        # Calculate the mean loss for the split and store it in the output dictionary\n",
    "        out[split] = np.mean(losses)\n",
    "    \n",
    "    # Set the model back to training mode\n",
    "    model.train()\n",
    "    \n",
    "    return out"
   ],
   "execution_count":null,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "node_id":"DdyJCdotjcsWc2UlTKJR1T",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "# Function to perform training\n",
    "def train(model, optimizer, config, scheduler=None, logs=False):\n",
    "    # Placeholder for storing losses\n",
    "    losses = []\n",
    "    \n",
    "    # Start tracking time\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Iterate through epochs\n",
    "    for epoch in range(config['epochs']):\n",
    "        # Zero out gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Obtain batches for training\n",
    "        xs, ys = get_batches(dataset, 'train', config['batch_size'], config['context_window'])\n",
    "\n",
    "        # Forward pass through the model to calculate logits and loss\n",
    "        logits, loss = model(xs, targets=ys)\n",
    "\n",
    "        # Backward pass and optimization step\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # If a learning rate scheduler is provided, adjust the learning rate\n",
    "        if scheduler:\n",
    "            scheduler.step()\n",
    "\n",
    "        # Log progress every specified interval\n",
    "        if epoch % config['log_interval'] == 0:\n",
    "            # Calculate batch time\n",
    "            batch_time = time.time() - start_time\n",
    "            \n",
    "            # Evaluate loss on validation set\n",
    "            x = evaluate_loss(model)\n",
    "            \n",
    "            # Store the validation loss\n",
    "            losses += [x]\n",
    "            \n",
    "            # Print progress logs if specified\n",
    "            if logs:\n",
    "                print(f\"Epoch {epoch} | val loss {x['val']:.3f} | Time {batch_time:.3f} | ETA in Secs {batch_time * (config['epochs'] - epoch)\/config['log_interval'] :.3f}\")\n",
    "                \n",
    "            # Reset the timer\n",
    "            start_time = time.time()\n",
    "\n",
    "            # Print learning rate if a scheduler is provided\n",
    "            if scheduler:\n",
    "                print(\"lr: \", scheduler.get_lr())\n",
    "\n",
    "    # Print the final validation loss\n",
    "    print(\"Validation loss: \", losses[-1]['val'])\n",
    "    \n",
    "    # Plot the training and validation loss curves\n",
    "    return pd.DataFrame(losses).plot()"
   ],
   "execution_count":null,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "node_id":"zzfQRFu8dWiSppUpEZioaL",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    
   ],
   "execution_count":null,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "node_id":"1hb6HcmOQ3RSxiSbly9KrX",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "# LLaMA introduces three architectural modifications to the original Transformer"
   ],
   "execution_count":null,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "node_id":"FMGiRs11qQJgb14V7Hc1fY",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "class RMSNorm(nn.Module):\n",
    "\n",
    "    def __init__(self, layer_shape):\n",
    "        super(RMSNorm, self).__init__()\n",
    "\n",
    "        # Registering a learnable parameter 'scale' as a parameter of the module\n",
    "        self.register_parameter(\"scale\", nn.Parameter(torch.ones(layer_shape)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Assumes shape is (batch, seq_len, d_model)\n",
    "        \"\"\"\n",
    "        # Calculating the Frobenius norm, RMS = 1\/sqrt(N) * Frobenius norm\n",
    "        ff_rms = torch.linalg.norm(x, dim=(1,2)) * x[0].numel() ** -.5\n",
    "\n",
    "        # Normalizing the input tensor 'x' with respect to RMS\n",
    "        raw = x \/ ff_rms.unsqueeze(-1).unsqueeze(-1)\n",
    "\n",
    "        # Scaling the normalized tensor using the learnable parameter 'scale'\n",
    "        return self.scale[:x.shape[1], :].unsqueeze(0) * raw"
   ],
   "execution_count":null,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "node_id":"FtubFaoEfQ8TNK6qnkqFO9",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "def get_rotary_matrix(context_window, embedding_dim):\n",
    "    # Initialize a tensor for the rotary matrix with zeros\n",
    "    r = torch.zeros((context_window, embedding_dim, embedding_dim), requires_grad=False)    \n",
    "    # Loop through each position in the context window\n",
    "    for position in range(context_window):\n",
    "        # Loop through each dimension in the embedding\n",
    "        for i in range(embedding_dim \/\/ 2):\n",
    "            # Calculate the rotation angle (theta) based on the position and embedding dimension\n",
    "            theta = 10000. ** (-2. * (i - 1) \/ embedding_dim)\n",
    "            # Calculate the rotated matrix elements using sine and cosine functions\n",
    "            m_theta = position * theta\n",
    "            r[position, 2 * i, 2 * i] = np.cos(m_theta)\n",
    "            r[position, 2 * i, 2 * i + 1] = -np.sin(m_theta)\n",
    "            r[position, 2 * i + 1, 2 * i] = np.sin(m_theta)\n",
    "            r[position, 2 * i + 1, 2 * i + 1] = np.cos(m_theta)\n",
    "\n",
    "    return r"
   ],
   "execution_count":null,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "node_id":"JlJRbCtDdgGnuNcoKvYJVT",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "class SwiGLU(nn.Module):\n",
    "    \n",
    "    def __init__(self, size):\n",
    "        super().__init__()\n",
    "        self.config = params  # Configuration information\n",
    "        self.linear_gate = nn.Linear(size, size)  # Linear transformation for the gating mechanism\n",
    "        self.linear = nn.Linear(size, size)  # Linear transformation for the main branch\n",
    "        self.beta = torch.randn(1, requires_grad=True)  # Random initialization of the beta parameter\n",
    "\n",
    "        # Using nn.Parameter for beta to ensure it's recognized as a learnable parameter\n",
    "        self.beta = nn.Parameter(torch.ones(1))\n",
    "        self.register_parameter(\"beta\", self.beta)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Swish-Gated Linear Unit computation\n",
    "        swish_gate = self.linear_gate(x) * torch.sigmoid(self.beta * self.linear_gate(x))\n",
    "        out = swish_gate * self.linear(x)  # Element-wise multiplication of the gate and main branch\n",
    "        return out"
   ],
   "execution_count":null,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "node_id":"7OM2lyxG8dfvT6IZPfDVeD",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    
   ],
   "execution_count":null,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "node_id":"4jfpQ6s8ic1d9qXO4VKfi0",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "# Create The Model"
   ],
   "execution_count":null,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "node_id":"oiD85QUkIvivFRc8by8jk6",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "class RopeAttentionHead(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        # Linear transformation for query\n",
    "        self.w_q = nn.Linear(config['d_model'], config['d_model'], bias=False)\n",
    "        # Linear transformation for key\n",
    "        self.w_k = nn.Linear(config['d_model'], config['d_model'], bias=False)\n",
    "        # Linear transformation for value\n",
    "        self.w_v = nn.Linear(config['d_model'], config['d_model'], bias=False)\n",
    "        # Obtain rotary matrix for positional embeddings\n",
    "        self.R = get_rotary_matrix(config['context_window'], config['d_model'])\n",
    "\n",
    "    def forward(self, x, return_attn_weights=False):\n",
    "        # x: input tensor of shape (batch, sequence length, dimension)\n",
    "        b, m, d = x.shape  # batch size, sequence length, dimension\n",
    "\n",
    "        # Linear transformations for Q, K, and V\n",
    "        q = self.w_q(x)\n",
    "        k = self.w_k(x)\n",
    "        v = self.w_v(x)\n",
    "\n",
    "        # Rotate Q and K using the RoPE matrix\n",
    "        q_rotated = (torch.bmm(q.transpose(0, 1), self.R[:m])).transpose(0, 1)\n",
    "        k_rotated = (torch.bmm(k.transpose(0, 1), self.R[:m])).transpose(0, 1)\n",
    "\n",
    "        # Perform scaled dot-product attention\n",
    "        activations = f.scaled_dot_product_attention(\n",
    "            q_rotated, k_rotated, v, dropout_p=0.1, is_causal=True\n",
    "        )\n",
    "\n",
    "        if return_attn_weights:\n",
    "            # Create a causal attention mask\n",
    "            attn_mask = torch.tril(torch.ones((m, m)), diagonal=0)\n",
    "            # Calculate attention weights and add causal mask\n",
    "            attn_weights = torch.bmm(q_rotated, k_rotated.transpose(1, 2)) \/ np.sqrt(d) + attn_mask\n",
    "            attn_weights = f.softmax(attn_weights, dim=-1)\n",
    "            return activations, attn_weights\n",
    "\n",
    "        return activations"
   ],
   "execution_count":null,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "node_id":"oDlVLppEE7TPgnIuWFHMbS",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "class RopeMaskedMultiheadAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        # Create a list of RopeAttentionHead instances as attention heads\n",
    "        self.heads = nn.ModuleList([\n",
    "            RopeAttentionHead(config) for _ in range(config['n_heads'])\n",
    "        ])\n",
    "        # Linear transformation after concatenating heads\n",
    "        self.linear = nn.Linear(config['n_heads'] * config['d_model'], config['d_model'])  \n",
    "        # Dropout layer\n",
    "        self.dropout = nn.Dropout(.1)  \n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: input tensor of shape (batch, sequence length, dimension)\n",
    "\n",
    "        # Process each attention head and concatenate the results\n",
    "        heads = [h(x) for h in self.heads]\n",
    "        x = torch.cat(heads, dim=-1)\n",
    "        \n",
    "        # Apply linear transformation to the concatenated output\n",
    "        x = self.linear(x)\n",
    "        \n",
    "        # Apply dropout\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        return x"
   ],
   "execution_count":null,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "node_id":"4ZTpl57PBkOub1yJufsOB2",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "# Add RMSNorm and residual connection\n",
    "class LlamaBlock(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        # RMSNorm layer\n",
    "        self.rms = RMSNorm((config['context_window'], config['d_model']))\n",
    "\n",
    "        # RoPE Masked Multihead Attention layer\n",
    "        self.attention = RopeMaskedMultiheadAttention(config)\n",
    "\n",
    "        # Feedforward layer with SwiGLU activation\n",
    "        self.feedforward = nn.Sequential(\n",
    "            nn.Linear(config['d_model'], config['d_model']),\n",
    "            SwiGLU(config['d_model']),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # one block of attention\n",
    "        x = self.rms(x) # RMS pre-normalization\n",
    "        x = x + self.attention(x)  # residual connection\n",
    "\n",
    "        x = self.rms(x) # RMS pre-normalization\n",
    "        x = x + self.feedforward(x)  # residual connection\n",
    "        return x"
   ],
   "execution_count":null,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "node_id":"MToNJc5pcPNyzFHrJ0Up0o",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "class Llama(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        # Embedding layer for token representations\n",
    "        self.embeddings = nn.Embedding(config['vocab_size'], config['d_model'])\n",
    "        # Sequential block of LlamaBlocks based on the specified number of layers\n",
    "        self.llama_blocks = nn.Sequential(\n",
    "            OrderedDict([(f\"llama_{i}\", LlamaBlock(config)) for i in range(config['n_layers'])])\n",
    "        )\n",
    "        # Feedforward network (FFN) for final output\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(config['d_model'], config['d_model']),\n",
    "            SwiGLU(config['d_model']),\n",
    "            nn.Linear(config['d_model'], config['vocab_size']),\n",
    "        )\n",
    "\n",
    "        # Print total number of parameters in the model\n",
    "        print(\"model params:\", sum([m.numel() for m in self.parameters()]))\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        # Input token indices are passed through the embedding layer\n",
    "        x = self.embeddings(idx)\n",
    "        # Process the input through the LlamaBlocks\n",
    "        x = self.llama_blocks(x)\n",
    "        # Pass the processed input through the final FFN for output logits\n",
    "        logits = self.ffn(x)\n",
    "\n",
    "        # If targets are not provided, return only the logits\n",
    "        if targets is None:\n",
    "            return logits\n",
    "        # If targets are provided, compute and return the cross-entropy loss\n",
    "        else:\n",
    "            loss = f.cross_entropy(logits.view(-1, self.config['vocab_size']), targets.view(-1))\n",
    "            return logits, loss"
   ],
   "execution_count":null,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "node_id":"c3tn12TtIKWwJIneEgaP7q",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    
   ],
   "execution_count":null,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "node_id":"1kxcMMmJ4jdJtmTgHT0ilI",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "#Train and Save the model.\n",
    "llama = Llama(params)\n",
    "adam_optimizer = torch.optim.Adam(llama.parameters())\n",
    "train(llama, adam_optimizer, params)\n",
    "torch.save(llama, 'llama_model.pth')\n",
    "\n",
    "#See Test Performance.\n",
    "txs, tys = get_batches(dataset, 'test', params['batch_size'], params['context_window'])\n",
    "tlogits, tloss = llama(txs, tys)\n",
    "print(tlogits)\n",
    "print(tloss)"
   ],
   "execution_count":null,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "node_id":"xmJVhhQYKD96NoKRQHD2Xj",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    
   ],
   "execution_count":null,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "node_id":"5wyc1SuokW7uibEU8YWKUV",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "# Create Llama model with Cosine Annealing learning schedule\n",
    "llama_with_cosine = Llama(params)\n",
    "\n",
    "# Define Adam optimizer with specific hyperparameters\n",
    "llama_optimizer = torch.optim.Adam(\n",
    "    llama.parameters(),\n",
    "    betas=(.9, .95),\n",
    "    weight_decay=.1,\n",
    "    eps=1e-9,\n",
    "    lr=1e-3\n",
    ")\n",
    "\n",
    "# Define Cosine Annealing learning rate scheduler\n",
    "lrscheduler = torch.optim.lr_scheduler.CosineAnnealingLR(llama_optimizer, 300, eta_min=1e-5)\n",
    "# Train the Llama model with the specified optimizer and scheduler\n",
    "train(llama_with_cosine, llama_optimizer, params, lrscheduler)\n",
    "torch.save(llama, 'llama_cos_model.pth')\n",
    "\n",
    "#See Test Performance.\n",
    "tlogits, tloss = llama_with_cosine(txs, tys)\n",
    "print(tlogits)\n",
    "print(tloss)"
   ],
   "execution_count":null,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "node_id":"WP96Ot9DpXWay3CKddI4VH",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "# Load the model and update so as to improve the number of paarmeters, fine tune it for your use case or\n",
    "# https:\/\/arxiv.org\/abs\/2005.11401\n",
    "\n",
    "# After the model learns english and depending on the use case, modifications to or another get_batch function as discussed.    "
   ],
   "execution_count":null,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "node_id":"zocNDlCRwaivTPN3qfVKYI",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    
   ],
   "execution_count":null,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "node_id":"9EHoWpMgP2lXjHIgg4RF69",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    
   ],
   "execution_count":null,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "node_id":"w8bO0zdD4Z6LGKG8mfBu9M",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  }
 ],
 "metadata":{
  "kernelspec":{
   "display_name":"Python",
   "language":"python",
   "name":"python"
  },
  "datalore":{
   "computation_mode":"JUPYTER",
   "package_manager":"pip",
   "base_environment":"default",
   "packages":[
    
   ],
   "report_row_ids":[
    
   ],
   "version":3
  }
 },
 "nbformat":4,
 "nbformat_minor":4
}